{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82aa094f-6fdf-4edd-bff2-95b44db7bd9e",
   "metadata": {},
   "source": [
    "# CORD-19 Project: Analysis of COVID-19 Papers\n",
    "The following notebook aims to perform a parallelized analysis on a dataset containing several COVID-19 related papers. This dataset is part of a real-world research on COVID-19 known as COVID-19 Open Research Dataset Challenge (CORD-19). Datasets and relative information can be retrieved here: https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge\n",
    "\n",
    "For this project we are going to use two version of the dataset: version 30 (13.63 GB) and version 50 (20.03 GB). The dataset is updated periodically and at present is composed by a staggering 87 GB of papers. We will make use of available resources on CloudVeneto to create a cluster, and Dask to perform the analysis in a parallelized setting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbadadbf-d55a-460c-8ea6-ba49e4135012",
   "metadata": {},
   "source": [
    "# 0: Setup of the cluster\n",
    "We created a cluster on CloudVeneto using 4 machines: one of them is the ```dask-scheduler```, the other 3 are ```dask-workers```. In addition to that, to guarantee consistent access to all machines to the dataset, we set up a NFS-server. We access the machines via ```ssh``` protocol.\n",
    "\n",
    "```bash\n",
    "ssh -L port:127.0.0.1:port -J username@gate.cloudveneto.it -i ~/.ssh/key.pem ubuntu@ip\n",
    "```\n",
    "\n",
    "We changed ```port```, ```username``` and ```ip``` based on our needs. Ip of our machines are:\n",
    "- ```10.67.22.173``` for the scheduler\n",
    "- ```10.67.22.150``` for worker 1\n",
    "- ```10.67.22.153``` for worker 2\n",
    "- ```10.67.22.183``` for worker 3\n",
    "- ```10.67.22.227``` for NFS server\n",
    "\n",
    "## Cluster creation\n",
    "\n",
    "We shared keys between the workers and the scheduler so they could communicate on their own.\n",
    "\n",
    "We used `miniconda` to exactly replicate the python environment on each VM.\n",
    "\n",
    "After some trials and errors we decided to store both dataset versions on a 60 GB volume. To grant access to every worker,\n",
    " we mounted the volume on a VM and configured it as an NFS server, following the CloudVeneto user's guide.\n",
    "\n",
    "To connect our machines to the NFS server we run on all of them:\n",
    "\n",
    "```bash\n",
    "sudo mount -t nfs 10.67.22.227:/home/ubuntu/nfs /home/ubuntu/data\n",
    "```\n",
    "\n",
    "Now we are ready to setup Dask with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c5007a-bba6-4c26-837d-f71af9637bd1",
   "metadata": {},
   "source": [
    "# 1: Libraries import and Cluster turn-on\n",
    "In this section we import all the libraries we will use in the notebook. Then we turn on the cluster via Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0ef7be-910a-45d8-8457-489936b38cdf",
   "metadata": {},
   "source": [
    "## 1.1: Libraries import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fb50f5-c22a-4df2-9950-8f6fc85b76a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, SSHCluster, default_client, PipInstall, performance_report, wait\n",
    "import ClusterManager as cm  ## Custom python script.\n",
    "import dask.bag as db\n",
    "import dask.dataframe as dd\n",
    "import dask\n",
    "import numpy as np\n",
    "import json\n",
    "from IPython.display import clear_output\n",
    "import glob as gl\n",
    "import os\n",
    "import glob\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import re\n",
    "import nltk\n",
    "from dask.distributed import Client\n",
    "import socket\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import time\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8f318b-0e53-4a69-9529-0a8f4f1e44a0",
   "metadata": {},
   "source": [
    "## 1.2: Cluster setup\n",
    "The following command starts off the as-before setupped cluster. To access the dashboard, we create a ```SSH``` tunnel via the command line:\n",
    "\n",
    "```bash\n",
    "ssh -L 8797:localhost:8797 -J ncognome@gate.cloudveneto.it -i ~/.ssh/chiave.pem ubuntu@10.67.22.173\n",
    "```\n",
    "\n",
    "So that we can then access the Dask Dashboard on http://localhost:8797"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918fedce-7872-4617-83f6-cf0e77c2abc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = cm.ClusterStarter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e245a0d-7233-454e-8a2a-5bafc2d30d03",
   "metadata": {},
   "source": [
    "We can check the status of the cluster by giving a simple task to perform. We should see the activity log activating in the Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8a9a13-7bfc-40f5-b77d-7fad1ab03687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def who_am_i():\n",
    "    import socket\n",
    "    return socket.gethostname()\n",
    "\n",
    "futures = [client.submit(who_am_i) for _ in range(10)]\n",
    "results = client.gather(futures)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6810bc34-8819-49d3-ad27-ab5589d977d6",
   "metadata": {},
   "source": [
    "# 2: Dataset setup\n",
    "In this part of the Notebook we focus on the available datasets and try to retrieve the correct information from each record. First off, we decide which version of the dataset to analyze and inspect a single JSON's structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c98b783-980b-4745-b336-7f1747bd858c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_version = 30  #pr 50/ whichever we prefer\n",
    "directory_path = f\"data/{dataset_version}/document_parses/pdf_json/\"\n",
    "filenames = gl.glob(directory_path + '*.json')\n",
    "\n",
    "print(f\"Found {len(filenames)} JSON files. \\n\")\n",
    "\n",
    "example = filenames[1]\n",
    "with open(example, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print('Each JSON is a dictionary with some keys:')\n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c10725-e21b-4530-9cbe-f21f3599f1f7",
   "metadata": {},
   "source": [
    "All of our files should have the following structure:\n",
    "```json\n",
    "{\n",
    "    \"paper_id\": <str>,                      # 40-character sha1 of the PDF\n",
    "    \"metadata\": {\n",
    "        \"title\": <str>,\n",
    "        \"authors\": [                        # list of author dicts, in order\n",
    "            {\n",
    "                \"first\": <str>,\n",
    "                \"middle\": <list of str>,\n",
    "                \"last\": <str>,\n",
    "                \"suffix\": <str>,\n",
    "                \"affiliation\": <dict>,\n",
    "                \"email\": <str>\n",
    "            },\n",
    "            ...\n",
    "        ],\n",
    "        \"abstract\": [                       # list of paragraphs in the abstract\n",
    "            {\n",
    "                \"text\": <str>,\n",
    "                \"cite_spans\": [             # list of character indices of inline citations\n",
    "                                            # e.g. citation \"[7]\" occurs at positions 151-154 in \"text\"\n",
    "                                            #      linked to bibliography entry BIBREF3\n",
    "                    {\n",
    "                        \"start\": 151,\n",
    "                        \"end\": 154,\n",
    "                        \"text\": \"[7]\",\n",
    "                        \"ref_id\": \"BIBREF3\"\n",
    "                    },\n",
    "                    ...\n",
    "                ],\n",
    "                \"ref_spans\": <list of dicts similar to cite_spans>,     # e.g. inline reference to \"Table 1\"\n",
    "                \"section\": \"Abstract\"\n",
    "            },\n",
    "            ...\n",
    "        ],\n",
    "        \"body_text\": [                      # list of paragraphs in full body\n",
    "                                            # paragraph dicts look the same as above\n",
    "            {\n",
    "                \"text\": <str>,\n",
    "                \"cite_spans\": [],\n",
    "                \"ref_spans\": [],\n",
    "                \"eq_spans\": [],\n",
    "                \"section\": \"Introduction\"\n",
    "            },\n",
    "            ...\n",
    "            {\n",
    "                ...,\n",
    "                \"section\": \"Conclusion\"\n",
    "            }\n",
    "        ],\n",
    "        \"bib_entries\": {\n",
    "            \"BIBREF0\": {\n",
    "                \"ref_id\": <str>,\n",
    "                \"title\": <str>,\n",
    "                \"authors\": <list of dict>       # same structure as earlier,\n",
    "                                                # but without `affiliation` or `email`\n",
    "                \"year\": <int>,\n",
    "                \"venue\": <str>,\n",
    "                \"volume\": <str>,\n",
    "                \"issn\": <str>,\n",
    "                \"pages\": <str>,\n",
    "                \"other_ids\": {\n",
    "                    \"DOI\": [\n",
    "                        <str>\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "            \"BIBREF1\": {},\n",
    "            ...\n",
    "            \"BIBREF25\": {}\n",
    "        },\n",
    "        \"ref_entries\":\n",
    "            \"FIGREF0\": {\n",
    "                \"text\": <str>,                  # figure caption text\n",
    "                \"type\": \"figure\"\n",
    "            },\n",
    "            ...\n",
    "            \"TABREF13\": {\n",
    "                \"text\": <str>,                  # table caption text\n",
    "                \"type\": \"table\"\n",
    "            }\n",
    "        },\n",
    "        \"back_matter\": <list of dict>           # same structure as body_text\n",
    "    }\n",
    "}\n",
    "```\n",
    "We have to handle them carefully, since there can be missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f801b33-fd39-438f-bca9-aab1b1019602",
   "metadata": {},
   "source": [
    "## 2.1: Dask bag\n",
    "Dask bags are one of the available data structures on Dask. They provide very generalized computations, and interpret their content as lists (the i-th element can be accessed via command, and also there can be duplicated arguments). For the first algorithm, this is our data structure of choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40757a6f-e960-43a9-8c8f-5c8b41266d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_partitions = 200\n",
    "files_to_take = len(filenames)\n",
    "filenames = filenames[:files_to_take]\n",
    "\n",
    "# function to load files\n",
    "def load_json_file(path):\n",
    "    with open(path) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "json_bag = db.from_sequence(filenames, npartitions=n_partitions).map(load_json_file)\n",
    "count = json_bag.count().compute()\n",
    "print('JSON bag contains',count,'files')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b6bf77-5e74-49aa-a4b1-28d5fe2c1ddc",
   "metadata": {},
   "source": [
    "# 3: Word counter distributed algorithm\n",
    "Our first real task is about a Word counter. Our goal is to determine how many times a single word appears in every document (and then in the full dataset). To achieve this we setup an algorithm divided in two phases:\n",
    "\n",
    "**Map phase**: For each document $D_i$ we will produce a set of intermediate pairs $(w,cp(w))$, one per each word $w$ in a given document $D_i$, where $cp(w)$ refers to the number of occurrences of $w$ in $D_i$.\n",
    "\n",
    "**Reduce phase**: For each word $w$, gather all pairs previously computed and return a final pair $(w,c(w))$ where $c(w)$ refers to the **total** number of occurrences of $w$ across all documents.  \n",
    "\n",
    "The algorithm will be executed on the full text of all the papers. To get the full text, we first need to access the ```body-text``` attribute of each JSON and concatenate the strings in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8366329-a846-44a0-9273-fd5b0a83fe3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# english common stopwords\n",
    "en_stopwords = stopwords.words(\"english\")\n",
    "\n",
    "# function to compute the words in a text\n",
    "def words_in_body(body_text):\n",
    "\n",
    "    # clean text\n",
    "    string = \" \".join([txt['text'] for txt in body_text]).lower() # join all paragraphs in lowercase\n",
    "\n",
    "    string = re.sub(r\"[^a-z\\s]\", \" \", string) # remove numbers and punctuations\n",
    "\n",
    "    tokens = string.split() # tokenize on whitespaces (avoids words splitting)\n",
    "\n",
    "    # remove stopwords, common words and single letters\n",
    "    stop_words = set(en_stopwords) | {\"fig\",\"figure\",\"et\",\"al\",\"results\",\"also\",\n",
    "                                      \"used\",\"using\",\"may\",\"one\",\"two\",\"de\",\"however\"}\n",
    "    \n",
    "    stop_words |= set(\"p o i u y t r e w q l k j h g f d s a z x c v b n m\".split())\n",
    "    filtered_words = [w for w in tokens if w not in stop_words]\n",
    "\n",
    "    words, words_counts = np.unique(filtered_words, return_counts=True)\n",
    "\n",
    "    return [{\"word\": str(i) , \"n_counts\": int(j)} for i,j in zip(words,words_counts)]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa84a8c8-fb91-43fb-8f9c-a7ef052a153d",
   "metadata": {},
   "source": [
    "We separately perform map and reduce phases in the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238fe9b3-3db4-4624-8f50-4f4059adbead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract each document's body text\n",
    "body_texts = json_bag.pluck(\"body_text\")\n",
    "\n",
    "# count words (map phase)\n",
    "words_counts = body_texts.map(words_in_body)\n",
    "\n",
    "# increment counters through all documents\n",
    "def increment(tot, x):\n",
    "    return tot + x['n_counts']\n",
    "\n",
    "# using foldby to get all occurrences of each 'word' (reduce phase)\n",
    "words_counted_reduce = words_counts.flatten().foldby('word', \n",
    "                                                     binop=increment, \n",
    "                                                     initial=0, \n",
    "                                                     combine = lambda x,y: x+y, \n",
    "                                                     combine_initial=0\n",
    "                                                    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55520550-6ef4-4a38-8090-3074b2a3f2b3",
   "metadata": {},
   "source": [
    "Dask is **lazy**: it does not perform any computation if it is not explicily said to do so. We compute the task and show the results in a histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327dcfb5-a271-4ba1-adb8-0e136c69314e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to a list of (word, count) tuples and compute\n",
    "word_counts = words_counted_reduce.compute()\n",
    "\n",
    "# sort and get highest 20\n",
    "sorted_words = sorted(word_counts, key=lambda x: x[1], reverse=True)[:20]\n",
    "words, counts = zip(*sorted_words)\n",
    "\n",
    "# plot logic\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(words, counts)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Top 20 Most Frequent Words\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a23850d-a01f-4934-8aea-4388207d4e94",
   "metadata": {},
   "source": [
    "## 3.1: Grid search on hyperparameters\n",
    "We want to be able to perform tasks with the highest speed possible. Hyperparameter configuration is strictly dependent on the dataset and its structure, and it's hard to infer a-priori what values they should take. For this reason, we repeat the previous analysis changing the number of available workers and the number of partitions, to see which combination performs best. We expect to have better results with 3 workers, and a number of partitions that is manageble by our workers, but at the same time that include a high number of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e88536-3a74-4ca1-a3a9-17be5f1a114f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configuration_time(n_partitions):\n",
    "    # take the starting time\n",
    "    start_time = time.time()\n",
    "    # configure the bag\n",
    "    json_bag = db.from_sequence(filenames, npartitions=n_partitions).map(load_json_file)\n",
    "    # perform the count(map phase)\n",
    "    words_counts = json_bag.pluck(\"body_text\").map(words_in_body)\n",
    "    # merge the counts (reduce phase)\n",
    "    words_counts_reduce = words_counts.flatten().foldby('word', \n",
    "                                                     binop=increment, \n",
    "                                                     initial=0, \n",
    "                                                     combine = lambda x,y: x+y, \n",
    "                                                     combine_initial=0\n",
    "                                                    ).compute() # here we compute to really perform the operation\n",
    "    end_time = time.time()\n",
    "    diff_time = end_time - start_time\n",
    "    print(f'With {n_partitions} partitions, computation time: {round(diff_time,2)}s')\n",
    "\n",
    "    # delete the objects (save space)\n",
    "    del json_bag\n",
    "    del words_counts\n",
    "    del words_counts_reduce\n",
    "    \n",
    "    return diff_time\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88856c6c-fc46-44b4-b22d-90b2c919a4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure that the cluster is closed\n",
    "client.close()\n",
    "cluster.close()\n",
    "# setup DataFrame for saving results\n",
    "df_times = pd.DataFrame(np.nan, index=partitions_list, columns=possible_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9e2f46-e05b-4080-ad24-e1a0b537f7ec",
   "metadata": {},
   "source": [
    "We ran the following cell a number of times to get all the data needed for completing the grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7683fcbc-8c7e-4f23-9689-335974922f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "partitions_list = [1,10, 20, 50, 100, 500]\n",
    "possible_workers = [1,2,3]  \n",
    "chosen_n_workers = possible_workers[0]\n",
    "\n",
    "client = cm.ClusterManager(nworkers=chosen_n_workers)\n",
    "\n",
    "# get the time for each partition count\n",
    "times_list = []\n",
    "print(f'\\nTrying {num_workers} workers...')\n",
    "for partitions in partitions_list:\n",
    "    t = configuration_time(partitions)\n",
    "    times_list.append(t)\n",
    "\n",
    "df_times[chosen_n_workers] = times_list\n",
    "\n",
    "# close client and cluster \n",
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e937ac86-e35f-4a6e-97f3-4391629652b6",
   "metadata": {},
   "source": [
    "When the grid is complete, we plot it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903c29d3-4a74-4f0b-9ee1-dcb2ad83ed82",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(final_df, annot=True, fmt=\".2f\", cmap=\"jet\", alpha = 0.8)\n",
    "plt.title(\"Execution Time Heatmap\")\n",
    "plt.xlabel(\"Number of Workers\")\n",
    "plt.ylabel(\"Number of Partitions\")\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"Grid_search_execution_times.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753e05bc-ecfb-4f1c-ba57-bd698cf62d13",
   "metadata": {},
   "source": [
    "# 4: Identifying the worst and best represented countries/institutes in the research\n",
    "For this second task we will convert our dataset into ```DataFrame``` structure. Our aim is to identify the most and least active countries in this research field. The analysis is based on affiliation institute of authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5085e7e5-5c36-4d5c-8a1e-c1b7da72e318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking data\n",
    "client = cm.ClusterManager(nworkers=chosen_n_workers)\n",
    "\n",
    "directory_path = f\"data/{dataset_version}/document_parses/pdf_json/\"\n",
    "filenames = gl.glob(directory_path + '*.json')\n",
    "\n",
    "print(f\"Found {len(filenames)} JSON files. \\n\")\n",
    "\n",
    "with open(example, 'r') as f:\n",
    "    data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745e200e-902e-428b-8e47-9004f601e79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def extract_key_fields(filepath):\n",
    "    with open(filepath) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    authors = data.get('metadata', {}).get('authors', [])\n",
    "    # extract information in dict\n",
    "    return {\n",
    "        'paper_id': data.get('paper_id'),\n",
    "        'title': data.get('metadata', {}).get('title'),\n",
    "        'author_count': len(data.get('metadata', {}).get('authors', [])),\n",
    "        'affiliations': [author.get('affiliation') for author in authors],\n",
    "        'first_author': data.get('metadata', {}).get('authors', [{}])[0].get('last') if data.get('metadata', {}).get('authors') else None,\n",
    "            }\n",
    "\n",
    "# create the dataframe\n",
    "npartitions = max(1, len(filenames) // 50)  \n",
    "json_bag2 = db.from_sequence(filenames, npartitions=npartitions).map(extract_key_fields)\n",
    "json_df = json_bag2.to_dataframe()\n",
    "\n",
    "count = json_df.count().compute()\n",
    "print('JSON Data Frame contains', count, 'papers')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12711a4-191b-4901-8ed7-311f41251577",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15572675-e26a-487f-90d3-b7aa405e56b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def process_partition(partition):\n",
    "    all_institutions = []\n",
    "    all_countries = []\n",
    "    \n",
    "    for affiliations in partition['affiliations']:\n",
    "        if isinstance(affiliations, list):\n",
    "            for aff in affiliations:\n",
    "                if isinstance(aff, dict):\n",
    "                    if 'institution' in aff:\n",
    "                        all_institutions.append(aff['institution'])\n",
    "                    if 'location' in aff and isinstance(aff['location'], dict):\n",
    "                        if 'country' in aff['location']:\n",
    "                            all_countries.append(aff['location']['country'])\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'institutions': [all_institutions],\n",
    "        'countries': [all_countries]\n",
    "    })\n",
    "\n",
    "# Apply to each partition\n",
    "results = json_df.map_partitions(process_partition, meta={'institutions': object, 'countries': object})\n",
    "all_institutions = [item for sublist in results['institutions'] for item in results]\n",
    "all_countries = [item for sublist in results['countries'] for item in results]\n",
    "\n",
    "# Compute and aggregate\n",
    "computed = results.compute()\n",
    "\n",
    "all_institutions = [item for sublist in computed['institutions'] for item in sublist]\n",
    "all_countries = [item for sublist in computed['countries'] for item in sublist]\n",
    "\n",
    "institution_counts = Counter(all_institutions)\n",
    "country_counts = Counter(all_countries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb4e492-1c2d-4c1e-9b5b-c1e19f9ab930",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "institution_counts = Counter(all_institutions)\n",
    "country_counts = Counter(all_countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83816514-0ae9-4b06-8183-ee9dcd6c42c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "institution_counts.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa45bb7-1af2-4dcd-a602-d553d8f9e286",
   "metadata": {},
   "outputs": [],
   "source": [
    "institution_counts = Counter(all_institutions)\n",
    "country_counts = Counter(all_countries)\n",
    "\n",
    "# Removing null values\n",
    "\n",
    "institution_counts = Counter({k: v for k, v in institution_counts.items() \n",
    "                           if k and str(k).strip() and k != 'null'})\n",
    "country_counts = Counter({k: v for k, v in country_counts.items() \n",
    "                           if k and str(k).strip() and k != 'null'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05b827a-c139-4b78-a93d-ade6f05e6a3e",
   "metadata": {},
   "source": [
    "We noticed that a fair number of countries occurred with different names (for example United States of America and USA). We want to merge the counts relative to the same real countries, but cited with different names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112e875b-e2ce-4360-ab93-92d4777572c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame.from_dict(country_counts, orient='index', columns=['count'])\n",
    "df2['country'] = df2.index\n",
    "df2 = df2.reset_index(drop=True)\n",
    "\n",
    "# mapping duplicate countries\n",
    "country_mapping = {\n",
    "    'USA': 'United States',\n",
    "    'United States of America': 'United States',\n",
    "    'US': 'United States',\n",
    "    'USA, USA': 'United States',\n",
    "    'Usa': 'United States',\n",
    "    'Alabama': 'United States',\n",
    "    'New Jersey': 'United States',\n",
    "    'UK': 'United Kingdom',\n",
    "    'UK A R': 'United Kingdom',\n",
    "    'Great Britain': 'United Kingdom',\n",
    "    'PR China': 'China', \n",
    "    'People\\'s Republic of China': 'China',\n",
    "    'P.R. China': 'China',\n",
    "    'PR China': 'China',\n",
    "    'China, China': 'China',\n",
    "    'P. R. China': 'China',\n",
    "    'China A R': 'China',\n",
    "    'Republic of Korea': 'South Korea',\n",
    "    'Korea': 'South Korea',\n",
    "    'España': 'Spain',\n",
    "    'the Netherlands': 'The Netherlands',\n",
    "    'Taiwan, ROC': 'Taiwan',\n",
    "    'Taiwan ROC': 'Taiwan',\n",
    "    'Italia': 'Italy',\n",
    "    'Russian Federation': 'Russia', \n",
    "    'CANADA': 'Canada',\n",
    "    'australia': 'Australia',\n",
    "    'UAE': 'United Arab Emirates',\n",
    "    'France Key Words': 'France',\n",
    "    'Democratic Republic of the Congo': 'Democratic Republic of Congo', \n",
    "    'Congo': 'Democratic Republic of Congo', \n",
    "    'The Gambia': 'Republic of The Gambia'\n",
    "}\n",
    "\n",
    "df2['country_clean'] = df2['country'].replace(country_mapping)\n",
    "df2 = df2[df2['country_clean'].str.match(r'^[a-zA-Z\\s]+$')]\n",
    "\n",
    "df2 = df2.groupby('country_clean')['count'].sum().reset_index()\n",
    "df2 = df2.sort_values('count', ascending=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "#df2.head(30)\n",
    "df2 = df2[df2['count'] >= 12]\n",
    "df2.head(-30)\n",
    "\n",
    "#Top 30\n",
    "plt.figure(figsize=(15, 8))\n",
    "df2.head(30).plot(x='country_clean', y='count', kind='bar', figsize=(15, 8))\n",
    "plt.title('Top 30 Countries by Count')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Bottom 30 countries\n",
    "plt.figure(figsize=(15, 8))\n",
    "df2.tail(30).iloc[::-1].plot(x='country_clean', y='count', kind='bar', figsize=(15, 8))\n",
    "plt.title('Bottom 30 Countries by Count')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178f44c5-9008-4290-81c3-cf07ec97fed1",
   "metadata": {},
   "source": [
    "In the end we plot in histograms the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d68cd3-6dc4-4fb0-9853-7021f42edd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_n = 20\n",
    "top_n = 20\n",
    "top_institutions = institution_counts.most_common(top_n)\n",
    "bottom_institutions = institution_counts.most_common()[:-(bottom_n+1):-1]\n",
    "\n",
    "names_t, counts_t = zip(*top_institutions)\n",
    "names_b, counts_b = zip(*bottom_institutions)\n",
    "\n",
    "# Create bar plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(range(len(names_t)), counts_t)\n",
    "plt.xticks(range(len(names_t)), names_t, rotation=45, ha='right')\n",
    "plt.xlabel('Institution')\n",
    "plt.ylabel('Count')\n",
    "plt.title(f'Top {top_n} Institutions')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea389b5-2152-472b-8e36-e3ff53e4af85",
   "metadata": {},
   "source": [
    "# 5: Obtaining embeddings for Paper Titles\n",
    "In Natural Language Processing, a common tecnique for text analysis is to transform text into numerical vectors, where each vector represents a word in a document. At the end of this operation, the document is transformed in a list of vectors, or a matrix ```n x m``` where ```n``` is the number of words in the document and ```m``` is the size of the vector representing each word ```n```.\n",
    "\n",
    "For this part, we took a pre-trained English model. The model is a large dictionary whose entries are in the form ```key:vector```, where each ```key``` corresponds to a word. This way we can easily link each word to its own vector. We will map the dataset in a DataFrame composed of ```paper-id``` (```pid```) and ```title-embedding``` (```vec```), so we link each document's id to the representation of its title in the vector space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f867bbad-445d-4bd2-b2ee-a0bdebf4bb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  helper functions:\n",
    "def split_like_oleg(inputi:str)->list:\n",
    "    \"\"\"\n",
    "    splits and lowers string\n",
    "    \"\"\"\n",
    "    if type(inputi) != type('hey'): return []\n",
    "    return inputi.lower().split()\n",
    "\n",
    "def master_splinter(inputi:str) -> dict:\n",
    "    \"\"\"\n",
    "    Splits strings and turns them in dicts made like {'word': [vector]}\n",
    "    \"\"\"\n",
    "    line = inputi.rstrip().split(' ')\n",
    "    if len(line) < 301: return None # just to be sure\n",
    "    elif len(line[0]) <= 2: return None # remove header\n",
    "    else:\n",
    "        return dict({'word' :str(line[0]), 'vec': list(map(float,line[1:])) })\n",
    "\n",
    "def usgarrista(inputi):\n",
    "    \"\"\"\n",
    "    Returns True if  NoneType\n",
    "    \"\"\"\n",
    "    return inputi is None \n",
    "\n",
    "def list_or_nothing(objecto): # some words do not appear in model: annihilated\n",
    "    if objecto is None: return []\n",
    "    if isinstance(objecto, list): return objecto\n",
    "    return []\n",
    "\n",
    "# functions for this part\n",
    "def nlp_embedding(num_workers:int,\n",
    "                            title_pid_partitions:float,\n",
    "                            wiki_blksize:float,\n",
    "                            filt_partitions:float,\n",
    "                            v_dataset:int,\n",
    "                            benchmark:bool):\n",
    "    \"\"\"\n",
    "    This function takes the metadata.csv file, selects title and paper ID, extract a list of unique words from all the titles.\n",
    "    Then it filters the model wiki.en.vec (the biggest one from FastText) to have a model with just the words we need.\n",
    "    The it maps each word to the model to retrieve the embedding. A group by the index gives us a dataframe w/ ID + list of vectors.\n",
    "    \"\"\"\n",
    "    client = cm.ClusterStarter(nworkers=num_workers)\n",
    "    dataset_version= v_dataset\n",
    "    modelpath = '/home/ubuntu/data/NLP_model/wiki.en.vec' ## Model location\n",
    "    metadata_path = f'/home/ubuntu/data/{dataset_version}/metadata.csv' # metadata location\n",
    "\n",
    "    metadatacsv = dd.read_csv(metadata_path,   # Reading metadata.csv\n",
    "                            blocksize=32e6, ## so it is divided in 3,more partitions make no sense\n",
    "                            dtype={'title': str, 'sha': str, 'pmcid': str, 'arxiv_id': str, 'pubmed_id': str, 'who_covidence_id': str}\n",
    "                            ).loc[:,['sha', 'title','pmcid']]\n",
    "\n",
    "    metadatacsv['pid'] = metadatacsv['sha'].fillna(value=metadatacsv['pmcid']).astype(str) # sometimes pdf_jsonID is missing, we will use from PMC_json\n",
    "    title_pid = metadatacsv.drop(columns=['sha','pmcid']) # don't need those\n",
    "    title_pid['title'] = title_pid['title'].apply(split_like_oleg, meta=('title', 'object')) # turning every title-string into title-list\n",
    "    title_pid = title_pid.explode('title').repartition(npartitions=title_pid_partitions) # exploding every list + repartitioning\n",
    "\n",
    "\n",
    "    uni_words = dd.from_pandas(pd.DataFrame({'word':title_pid['title'].unique()})).astype({'word': str})#unique words in all titles\n",
    "\n",
    "    del metadatacsv \n",
    "    client.run(gc.collect) # naive memory management\n",
    "\n",
    "    wiki_vec = db.read_text(modelpath,# loading model\n",
    "                            blocksize=wiki_blksize,\n",
    "                            encoding='utf-8',\n",
    "                            errors='strict',\n",
    "                            linedelimiter='\\n',\n",
    "                            collection=True\n",
    "                            ).map(\n",
    "                                master_splinter\n",
    "                                ).remove(\n",
    "                                    usgarrista\n",
    "                                    ).to_dataframe(\n",
    "                                        meta= {'word': str, 'vec': 'object'}\n",
    "                                    )\n",
    "    wiki_vec['word'] = wiki_vec['word'].astype(str)\n",
    "\n",
    "    filtered_wiki = wiki_vec.merge(uni_words, # intersecting model with unique words, so we will use just what we need\n",
    "                                    on='word',\n",
    "                                    how='inner'\n",
    "                                    ).repartition(\n",
    "                                        npartitions=filt_partitions)\n",
    "    del wiki_vec, master_splinter, usgarrista, uni_words, modelpath, metadata_path # memory management\n",
    "    client.run(gc.collect)\n",
    "\n",
    "    the_word_and_friends = title_pid.merge(filtered_wiki, how='left', left_on='title',right_on='word') # merging exploded titles w/ filtered wiki\n",
    "    the_word_and_the_vec = the_word_and_friends[['pid','vec']].copy() # some cleaning/refining\n",
    "    the_word_and_the_vec['vec'].apply(list_or_nothing, meta=('vec', 'object'))\n",
    "    del the_word_and_friends # memory management\n",
    "    client.run(gc.collect)\n",
    "\n",
    "    nlp_bible = the_word_and_the_vec.groupby('pid')['vec'].apply(list, meta=('vec','object')).reset_index().persist()\n",
    "    \n",
    "    if benchmark:\n",
    "        wait(nlp_bible) # wait for persist to compute the df\n",
    "        del nlp_bible, the_word_and_the_vec # memory management of the cluster if benchmarking\n",
    "        client.run(gc.collect)\n",
    "        client.shutdown()\n",
    "        return 'worked'\n",
    "    else:\n",
    "        wait(nlp_bible) ## this is not really necessary but maybe it's better to have the dataset ready before starting the next task. We will decide after doing the last task.\n",
    "        return client, nlp_bible\n",
    "\n",
    "\n",
    "def benchmark():\n",
    "    worker_list = [3, 2, 1] # reversed because is faster to shut down a worker than waking it up\n",
    "    title_pid_parts = 3 # it does not makes sense to split this in more partitions than 3\n",
    "    wiki_blksize = [32e6, 64e6, 128e6] #,512e6,1e9] bigger partitions were not manageable by workers\n",
    "    filt_wiki = [3, 6, 9] # filt_wiki is like a third of wiki_blk size so ~ 2 GB, so these number of partitions should be ok. According to our resources and Dask docs.\n",
    "    times = [] \n",
    "    check = [] # check it not really needed but it makes me feel calmer during benchmarking.\n",
    "    param_list = []\n",
    "    # things to add? CPU Load? Data transfer? # not enough time left to implement it im afraid\n",
    "    for worker in worker_list:        \n",
    "        for blksz in wiki_blksize:\n",
    "            for filt_wikiii in filt_wiki:\n",
    "                start = time.time()\n",
    "                a = nlp_embedding(num_workers=worker,\n",
    "                    title_pid_partitions=title,\n",
    "                    wiki_blksize= blksz,\n",
    "                    filt_partitions=filt_wikiii, \n",
    "                    v_dataset=dataset_version,\n",
    "                        benchmark=True )\n",
    "                end = time.time()\n",
    "                diff_time = end - start\n",
    "                #times.append(diff_time)\n",
    "                #check.append(a)\n",
    "\n",
    "                params = {\n",
    "                    'workers': worker,\n",
    "                    'wiki_blksize': blksz,\n",
    "                    'title_pid_parts': title,\n",
    "                    'filt_wiki': filt_wikiii,\n",
    "                    'time': diff_time,\n",
    "                    'check': a\n",
    "                }\n",
    "                param_list.append(params)\n",
    "    \n",
    "    return param_list #times, check, params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32eba617-b5d7-4c0a-ad5a-fb2b83338101",
   "metadata": {},
   "source": [
    "## 5.1: Grid search on hyperparameters\n",
    "As previously done for the word counter algorithm, we want to look for the best setting that lets us perform the task as quickly as possible. The way we have set up the functions lets us set a flag that lets us perform a benchmark. Our parameter space is the following:\n",
    "\n",
    "```python\n",
    "worker_list = [3,2,1]\n",
    "title_pid_parts = [3, 6, 9]\n",
    "wiki_blksize = [32e6, 64e6, 128e6] #,512e6,1e9] \n",
    "filt_wiki = [3, 6, 9]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72cdea5-89b6-4543-91ad-d51b01a917cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = benchmark()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ec81b5-8c8c-4947-8f14-02289bc7cefb",
   "metadata": {},
   "source": [
    "Then we retrieve the best performance's time and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966d98ce-ab3b-4904-b1ef-4f2a4a4d4669",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = [x['time'] for x in params]\n",
    "times = np.array(times)\n",
    "best_time_index = np.argmin(times)\n",
    "best_time = times[best_time_index]\n",
    "best_params = params[best_time_index]\n",
    "\n",
    "print(f'We got best time: {best_time}s with params: \\n{best_params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc11c6f2-94d6-40fe-a33f-87609f305a96",
   "metadata": {},
   "source": [
    "## 5.2: Setup the best parameters and see the results\n",
    "We want to setup the cluster correctly for the last part and check the representation of the titles as vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba552384-0ec2-4b9f-855e-c26346c2f7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_workers = best_params['workers']\n",
    "title_pid_partitions = best_params['title_pid_parts']\n",
    "wiki_blksize = best_params['wiki_blksize']\n",
    "filt_partitions = best_params['filt_wiki']\n",
    "benchmark = False\n",
    "\n",
    "client, df = nlp_embedding(num_workers=n_workers,\n",
    "                 title_pid_partitions=title_pid_partitions,\n",
    "                  wiki_blksize=wiki,_blksize,\n",
    "                   filt_partitions=filt_partitions,\n",
    "                    v_dataset=dataset_version,\n",
    "                     benchmark=benchmark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d23f87c-f851-493a-951c-6d1b4f27d901",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1).iloc[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
